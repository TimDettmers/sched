/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
/usr/bin/bash: /private/home/timdettmers/anaconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
Traceback (most recent call last):
  File "/private/home/timdettmers/anaconda3/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 369, in cli_main
    distributed_utils.call_main(args, main)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 257, in call_main
    distributed_main(args.device_id, main, args, kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/distributed_utils.py", line 238, in distributed_main
    main(args, **kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq_cli/train.py", line 113, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/private/home/timdettmers/git/fairseq_private/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    trainer.lr_step(epoch_itr.epoch)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 811, in lr_step
    self.lr_scheduler.step(epoch, val_loss)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 195, in lr_scheduler
    self._build_optimizer()  # this will initialize self._lr_scheduler
  File "/private/home/timdettmers/git/fairseq_private/fairseq/trainer.py", line 240, in _build_optimizer
    self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/__init__.py", line 30, in build_lr_scheduler
    return build_lr_scheduler_(lr_scheduler_cfg, optimizer)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/private/home/timdettmers/git/fairseq_private/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py", line 83, in __init__
    assert self.max_lr > self.min_lr, "max_lr must be more than lr"
AssertionError: max_lr must be more than lr
srun: error: learnfair2393: task 3: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=37240580.0
slurmstepd: error: *** STEP 37240580.0 ON learnfair2393 CANCELLED AT 2021-03-12T21:36:14 ***
srun: error: learnfair2393: tasks 0-2,4-7: Terminated
srun: Force Terminated StepId=37240580.0
